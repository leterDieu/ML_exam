# Лекция 0. (образец)
>[!question]- 1. Семпл вопрос
>
>**Определение**
>
>  Текст определения
>
>  ***Список***
> 1. Строка один
>2. Строка два
>
> ***Список 2***
> - Строка
> - Строка
> 
> ***Таблица***
>>| Колонка 1 | Колонка 2   |
>| ------ | ------ |
>| 1      | 3 |
>| 2      | 4 |
>
> ***Код***
>```python
>print('hello world')
>```
>
> ***Формула***
> $$ 
> \frac{a}{b} = \sqrt{ 2 }
> $$

# Лекция 5. Бустинг и градиентный бустинг над деревьями
>[!question]- 1. Сформулировать общую идею бустинга. Чем бустинг принципиально отличается от бэггинга и стэкинга?
>
> **Общая идея**
> 
> Последовательное добавление слабых моделей, каждая из которых исправляет ошибки предыдущей, i.e. предсказывает ошибку предыдущей.
>
> ***Отличие от бэггинга и стэкинга***
>   
> При бэггинге обучется несколько моделей, и финальное предсказание формируется "голосованием" (берется мода или среднее от всех предсказаний)
> При стэкинге на предсказаниях нескольких моделей обучается еще одна модель, которая сможет лучше комбинировать их ответы, чем просто мода или среднее.
> При бустинге же модели работают как бы не параллельно, а последовательно. Каждая последующая модель, кроме первой, старается предсказать не таргет переменную, как в бэггинге или стэкинге, а ошибку предыдущей модели.
> Бэггинг и стэкинг менее чувствительны к отдельным выбросам. Бустинг чувствителен к шуму (важна регуляризация).

>[!question]- 2. Записать и_ли объяснить объяснить общее обновление модели в градиентном бустинге [1]
> $$ F_{M}(x) = F_{M-1}(x) + v*h_{m}(x) $$
> 
> Где 
> - $$ F_{0}(x) $$ - изначальная модель, которая грубо предсказывает ошибку
> - $$ h_{M}(x) $$ - модель, которая предсказывает ошибки $$ F_{M-1}(x) $$ 
> - $$ v \in (0; 1] $$ - скорость обучения (i.e. как сильно модель каждой итерации влияет на конечную модель. Чем меньше  скорость обучения, тем больше нужно обучить моделей, но тем более точно можно скорректировать финальную модель)
> 
> Оно же:
> 
> $$ F(x) = F_{0}(x) + \sum_{i=1}^{h} h_{i}(x) * v^i $$

>[!question]- 3. Пояснить точку зрения градиентного спуска в пространстве функций: как задается функционал ошибки L(F) и какую роль играют псевдо-остатки (антиградиенты) на обучающих объектах.
>
> **Пространство функций**
>
> Пространство функций - это множество слабых функций h, которые мы перебираем для нахождения лучшей финальной функции F
>
> **Как задается функционал ошибка L(F)**
> 
> $$ L(F) = \sum_{i=1}^{n}l(y_{i}, F(x_{i})) $$
> 
> Где F - искомая функция, а l - выбранная функция потерь.
> 
>  **Псевдо-остатки**
>  
>  На каждом шаге нам нужна модель h, которая будет предсказывать ошибку прошлой модели F. 
> 
>  $$ g_{im} = \frac{\partial l(y_{i}, F_{m-1}(x_{i}))}{\partial F_{m-1}(x_{i})} $$
>  
>  $$ h_{m} \approx -g_{im} $$
>  
>  I.e. псевдо-остатки (производная Фреше в обобщенном метрическом пространстве) пропорциональны предсказаниям модели h.
>  
>  Градиентный бустинг называется так, т.к. формула сильно похожа на формулу градиентного спуска
>  
>  $$ x = x_{0} - \sum_{i=1}^{n} v_{i} g_{'}(x_{i}) $$

> [!question]- 4. Записать и_ли объяснить шаг алгоритма градиентного бустинга над деревьями: вычисление псевдо-остатков, обучение регрессионного дерева по этим значениям, поиск оптимальных сдвигов по листам и обновление ансамбля [1]
> 
> 1. Вычисление псевдо-остатков $$ r_{im} = \frac{\partial l(y_{i}, F_{m-1}(x_{i}))}{\partial F_{m-1}(x_{i})} $$
> 2. Обучение регрессионного дерева h_m(x) по парам (x_i, r_im)
> 3. Нахождение оптимального сдвига (константы) по листам: для каждого листа R с индексом jm $$ γ_{jm} = arg\min_{γ} \sum_{x_{i} \in R_{jm}} l(y_{i}, F_{m-1}(x_{i}) + γ) $$
> 4. Обновление модели $$ F_{m}(x) = F_{m-1}(x) + v * \sum_{j} γ_{jm} 1\{x \in R_{jm} \} $$

>[!question]- 5. Перечислить основные способы регуляризации в градиентном бустинге: малая скорость обучения v, ограничение глубины деревьев, минимальный размер листа, субсемплинг объектов, субсемплинг признаков, ранняя остановка по валидационной выборке.
>
> 1. Скорость обучения (shrinkage)  0 < v <= 1. Чем меньше, тем устойчивее, но нужно больше деревьев.
> 2. Глубина дерева / число листьев: неглубокие деревья (3-8 уровней) приводят к слабым базовым моделям.
> 3. Субсемплинг объектов: обучение h_m на случайной доле данных (обычно от 0.5 до 0.9)
> 4. Субсемплинг признаков: случайный поднабор признаков на сплите/уровне/дереве.
> 5. Минимальный размер листа, L2-штраф на веса листок, макс. число узлов.
> 6. Ранняя остановка по валидации: мониторинг метрики и прекращение роста M (перетекает в некст вопрос)

>[!question]- 6. Объяснить, как по поведению метрик на обучающей и валидационной выборках диагностировать переобучение бустинговой модели и как выбирать оптимальное число итераций.
>
> **Переобучение**
> 
> При подборе гиперпараметров нужно стремиться к уменьшению метрики на валидации и сохранению метрики на обучающей выборке. Если материка на обучающей выборке уменьшается, а на валидационной - нет, то это признак переобучения.
> 
> **Число итераций**
> 
> Подбирается так же, как и любой гиперпараметр, - по метрике валидационной выборке
